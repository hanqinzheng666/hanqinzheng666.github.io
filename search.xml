<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于JWT的token身份认证]]></title>
    <url>%2F2019%2F10%2F21%2F%E5%9F%BA%E4%BA%8EJWT%E7%9A%84token%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[基于JWT的token身份认证一、JWT(JSON Web Token)是什么？JWT是基于token的身份认证的方案，全称json web token。可以保证安全传输的前提下传送一些基本的信息，以减轻对外部存储的依赖，减少了分布式组件的依赖，减少了硬件的资源。 可实现无状态、分布式的Web应用授权，jwt的安全特性保证了token的不可伪造和不可篡改。 本质上是一个独立的身份验证令牌，可以包含用户标识、用户角色和权限等信息，以及您可以存储任何其他信息（自包含）。任何人都可以轻松读取和解析，并使用密钥来验证真实性。 缺陷： 1）JWT在生成token的时候支持失效时间，但是支持的失效时间是固定的，比如说一天，用户在等出的时候是随机触发的，那么我们jwt token来做这个失效是不可行的，因为jwt在初始化的时候已经定死在什么时候过期了。采用其他方案，在redis中存储token，设置token的过期时间，每次鉴权的时候都会去延长时间 2）jwt不适合存放大量信息，信息越多token越长 JWT包含三部分数据： Header：头部，通常头部有两部分信息： 声明类型，这里是JWT 加密算法，自定义 我们会对头部进行base64加密（可解密），得到第一部分数据 Payload：载荷，就是有效数据，一般包含下面信息： 用户身份信息（注意，这里因为采用base64加密，可解密，因此不要存放敏感信息） 注册声明：如token的签发时间，过期时间，签发人等 这部分也会采用base64加密，得到第二部分数据 Signature：签名，是整个数据的认证信息。一般根据前两步的数据，再加上服务的的密钥（secret）（不要泄漏，最好周期性更换），通过加密算法生成。用于验证整个数据完整和可靠性 生成的数据格式： 二、使用JWT(JSON Web Token)的好处？1.性能问题。JWT方式将用户状态分散到了客户端中，相比于session，可以明显减轻服务端的内存压力。 Session方式存储用户id的最大弊病在于Session是存储在服务器端的，所以需要占用大量服务器内存， 对于较大型应用而言可能还要保存许多的状态，一般还需借助nosql和缓存机制来实现session的存储，如果是分布式应用还需session共享。 2.单点登录。JWT能轻松的实现单点登录，因为用户的状态已经被传送到了客户端。 token 可保存自定义信息，如用户基本信息，web服务器用key去解析token，就获取到请求用户的信息了。 我们也可以配置它以便包含用户拥有的任何权限。这意味着每个服务不需要与授权服务交互才能授权用户。 3.前后端分离。以前的传统模式下，后台对应的客户端就是浏览器，就可以使用session+cookies的方式实现登录， 但是在前后分离的情况下，后端只负责通过暴露的RestApi提供数据，而页面的渲染、路由都由前端完成。因为rest是无状态的，因此也就不会有session记录到服务器端。 4.兼容性。支持移动设备，支持跨程序调用，Cookie 是不允许垮域访问的，而 Token 则不存在这个问题。 5.可拓展性。jwt是无状态的，特别适用于分布式站点的单点登录（SSO）场景。 比如有3台机器（A、B、C）组成服务器集群，若session存在机器A上，session只能保存在其中一台服务器，此时你便不能访问机器B、C，因为B、C上没有存放该Session， 而使用token就能够验证用户请求合法性，并且我再加几台机器也没事，所以可拓展性好。 6.安全性。因为有签名，所以JWT可以防止被篡改。 三、JWT(JSON Web Token)工作原理 初次登录：用户初次登录，输入用户名密码 密码验证：服务器从数据库取出用户名和密码进行验证 生成JWT：服务器端验证通过，根据从数据库返回的信息，以及预设规则，生成JWT 返还JWT：服务器的将token放在cookie中将JWT返还 带JWT的请求：以后客户端发起请求，带上cookie中的token信息。 四、jwt+redis的登录方案流程： 前端服务器收到用户登录请求，传给后台API网关。 API网关把请求分发到用户服务里进行身份验证。 后台用户服务验证通过，然后从账号信息抽取出userName、login_time等基本信息组成payload，进而组装一个JWT，把JWT放入redis(因为退出的时候无法使jwt立即作废，所以使用保存在redis中，退出的时候delete掉就可以了，鉴权的时候加一层判断jwt是否在redis里，如果不在则证明jwt已过期作废)，然后包装cookie中返回到前端服务器，这就登录成功了。 前端服务器拿到JWT，进行存储（可以存储在缓存中，也可以存储在数据库中，如果是浏览器，可以存储在 localStorage 中，我实现的是放入到cookie里面） 登录后，再访问其他微服务的时候，前端会携带jwt访问后台，后台校验 JWT，验签通过后，返回相应资源和数据就可以了。 （这里没有将redis画出来） 结合拦截器与上篇session-cookie方式的区别 首次登录步骤： 1.首先AuthInterceptor拦截器拦截用户请求，在preHandle中看cookie中是否有token信息，没有就接着拦截器AuthActionInterceptor拦截需要登录的url，看threadlocal当中是否有user对象，如果没有就跳转到登录页面进行登录，登录成功后会将user对象放到threadlocal中。（注意这个地方和普通登录成功后将user放到session的不同） 登录处理流程：在数据库中查询验证用户名密码，通过就讲账号信息抽取出username、email等信息组成一个payload，进而组装成一个JWT，然后将JWT放到redis当中，设置过期时间。 生成token：给定签名算法、给定载荷的map、进行签名 2.当业务逻辑处理完之后在AuthInterceptor的postHandle中，从threadlocal获取user对象中的token信息，将token放到cookie中返回给前端。 3.请求结束后在AuthInterceptor的afterCompletion将user从threadlocal中移除。 验证流程： 前端将携带jwt的cookie传到后台，AuthInterceptor会根据token验证解析出user，（注意根之前在session中取对象的不同）验证后再将user放到threadlocal中，AuthActionInterceptor一看threadlocal有user对象，直接通过。后面的步骤一样。 参考：https://www.cnblogs.com/xiangkejin/archive/2018/05/08/9011119.html]]></content>
      <categories>
        <category>JWT</category>
      </categories>
      <tags>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spring的邮件封装类JavaMailSenderImpl发送邮件]]></title>
    <url>%2F2019%2F10%2F08%2F%E4%BD%BF%E7%94%A8Spring%E7%9A%84%E9%82%AE%E4%BB%B6%E5%B0%81%E8%A3%85%E7%B1%BBJavaMailSenderImpl%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[使用Spring的邮件封装类JavaMailSenderImpl发送邮件前言JavaMailSenderImpl是Spring封装的邮件发送封装类，支持普通文本、附件、html。SpringBoot实现邮件功能是非常的方便快捷的，因为SpringBoot默认有starter实现了Mail。发送邮件应该是网站的必备功能之一，什么注册验证，忘记密码或者是给用户发送营销信息。最早期的时候我们会使用JavaMail相关api来写发送邮件的相关代码，后来spring退出了.JavaMailSender更加简化了邮件发送的过程，在之后springboot对此进行了封装就有了现在的spring-boot-starter-mail。 一、使用163.com发送邮件 开启POP3/SMTP/IMAP服务 开启并设置邮箱授权码 引入Spring相对应的包 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 在application.properties配置相对应的参数 163邮箱的配置 123456spring.mail.host=smtp.163.comspring.mail.username=用户163邮箱spring.mail.password=163邮箱授权码，第二步设置的spring.mail.properties.mail.smtp.auth=truespring.mail.properties.mail.smtp.starttls.enable=truespring.mail.properties.mail.smtp.starttls.required=true qq邮箱的配置 123456spring.mail.host=smtp.qq.comspring.mail.username=用户qq邮箱spring.mail.password=qq邮箱授权码，第二步设置的spring.mail.properties.mail.smtp.auth=truespring.mail.properties.mail.smtp.starttls.enable=truespring.mail.properties.mail.smtp.starttls.required=true 阿里云邮箱的设置 123456spring.mail.host=smtp.aliyun.comspring.mail.username=用户阿里云邮箱spring.mail.password=注意：阿里云密码，不是授权码spring.mail.properties.mail.smtp.auth=truespring.mail.properties.mail.smtp.starttls.enable=truespring.mail.properties.mail.smtp.starttls.required=true 发送简单格式的邮件 1234567891011121314@Autowiredprivate JavaMailSender mailSender;@Value("$&#123;spring.mail.username&#125;")private String from;public void setMail(String title, String text, String email) &#123; SimpleMailMessage message = new SimpleMailMessage(); message.setFrom(from); message.setTo(email); message.setSubject(title); message.setText(text); mailSender.send(message); &#125; 测试发送简单的邮件 12345678@Testpublic void TestSendMail()&#123; try&#123; mailService.setMail("邮件测试发送","很开心能发送邮件给你",需要发送的邮箱账号); &#125;catch (Exception e)&#123; logger.error("邮件发送失败",e.getMessage()); &#125; &#125; 发送一个HTML格式的邮件 1234567891011121314public void sendHTMLMail(String recipient,String subject,String content) &#123; MimeMessage mimeMailMessage = null; try &#123; mimeMailMessage = mailSender.createMimeMessage(); MimeMessageHelper mimeMessageHelper = new MimeMessageHelper(mimeMailMessage, true); mimeMessageHelper.setFrom(from); mimeMessageHelper.setTo(recipient); mimeMessageHelper.setSubject(subject); mimeMessageHelper.setText(content, true); mailSender.send(mimeMailMessage); &#125; catch (Exception e) &#123; logger.error("邮件发送失败",e.getMessage()); &#125;&#125; 测试发送HTML邮件 12345678@Testpublic void TestSendMail()&#123; StringBuilder sb = new StringBuilder(); sb.append("&lt;h1&gt;SpirngBoot测试邮件HTML&lt;/h1&gt;") .append("\"&lt;p style='color:#F00'&gt;你是真的太棒了！&lt;/p&gt;") .append("&lt;p style='text-align:right'&gt;右对齐&lt;/p&gt;"); mailService.sendHTMLMail(邮箱账号,"邮件测试发送",sb.toString()); &#125; 发送带有附件格式的邮件 123456789101112131415161718public void sendAttachmentMail(String recipient,String subject,String content) &#123; MimeMessage mimeMailMessage = null; try &#123; mimeMailMessage = mailSender.createMimeMessage(); MimeMessageHelper mimeMessageHelper = new MimeMessageHelper(mimeMailMessage, true); mimeMessageHelper.setFrom(from); mimeMessageHelper.setTo(recipient); mimeMessageHelper.setSubject(subject); mimeMessageHelper.setText(content, true); //文件路径 FileSystemResource file = new FileSystemResource(new File(""C:\idea_workspace\house\house-web\src\main\resources\static\imgs\property-04.jpg")); mimeMessageHelper.addAttachment(file.getFilename(), file); mailSender.send(mimeMailMessage); &#125; catch (Exception e) &#123; logger.error("邮件发送失败",e.getMessage()); &#125;&#125; 测试发送带有附件格式的图片邮件 1234@Testpublic void TestSendMail()&#123; mailService.sendAttachmentMail(邮箱账号,"邮件测试发送","我的图片");&#125;]]></content>
      <categories>
        <category>JavaMailSenderImpl</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>JavaMailSenderImpl</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot的配置文件]]></title>
    <url>%2F2019%2F09%2F26%2FSpring%20Boot%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Spring Boot的配置文件配置文件的两种方式SpringBoot使用一个全局的配置文件，配置文件名是固定的 application.properties application.yml 当我们创建一个springboot项目的时候，系统会默认在src/main/java/resources目录下创建一个application.properties。但是你也可以修改为application.yml 优先级 spring boot项目中同时存在application.properties和application.yml文件时，两个文件都有效，但是application.properties的优先级会比application.yml高。 配置文件所在目录不同优先级也不同。如下图1~4优先级从高到低，如下图 yml语法基本语法 k:(空格)v：表示一对键值对（空格必须有）； 以空格的缩进来控制层级关系；只要是左对齐的一列数据，都是同一个层级的 “”：双引号；不会转义字符串里面的特殊字符；特殊字符会作为本身想表示的意思 1name: "zhangsan \n lisi"：输出；zhangsan 换行 lisi ”：单引号；会转义特殊字符，特殊字符最终只是一个普通的字符串数据 1name: ‘zhangsan \n lisi’：输出；zhangsan \n lisi 具体的写法如下(注释的#代表的是application写法，没注释的#代表的yml写法) 123456789101112131415161718192021222324person: lastName: 张三 age: 24 boss: false birth: 1994/5/1# maps: &#123;k1: v1,k2: v2&#125; maps: k1: v1 k2: v2# lists: [l1,l2,l3] lists: - l1 - l2 - l3 dog: name: 蛋黄 age: 2server: port: 8090 数据绑定ConfigurationProperties123456789101112131415161718192021222324252627282930/***将配置文件中配置的每一个属性的值，映射到这个组件*@ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定* prefix = "person"：配置文件中哪个下面的所有属性进行一一映射* 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能*/@Component@Validated //数据校验@ConfigurationProperties(prefix = "person")public class Person &#123; private String favorite; private String hate; private String name; //检测值是否为Email格式 @Email private String email; private Date birth; private int age; private List&lt;String&gt; address; private Map&lt;String,String&gt; childrens; private Dog dog; 我们可以导入配置文件处理器，以后编写配置就有提示了 123456&lt;!‐‐导入配置文件处理器，配置文件进行绑定就会有提示‐‐&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring‐boot‐configuration‐processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; Value12345678910111213141516171819//从配置文件获取值 /** *相当于spring的bean配置 *&lt;bean class="Person"&gt; * &lt;property name="lastName" value="字面量/$&#123;key&#125;从环境变量、配置文件中获取值/# &#123;SpEL&#125;"&gt;&lt;/property&gt; * &lt;bean/&gt; * */ @Value("$&#123;person.name&#125;") private String name; //字面值常量 @Value("2017/10/10") private Date birth; //支持SPEL表达式 @Value("#&#123;2+5&#125;") private int age; @Value获取值和@ConﬁgurationProperties获取值比较 Value ConﬁgurationProperties 功能 一个个指定 批量注入配置文件中的属性 松散绑定（松散语法） 不支持 支持 SpEL 支持 不支持 JSR303 不支持 支持 复杂类型封装 不支持 支持 @PropertySource&amp;@ImportResource&amp;@Bean我们把所有的配置都写在默认的配置文件中未免过于沉重,所以我们可以使用@PropertySource 加载指定的配置文件 我们可以把我们person的配置写在person.properties文件中 1234@PropertySource(value = &#123;"classpath:person.properties"&#125;)@Component@ConfigurationProperties(prefix = "person")public class Person &#123; @ImportResource：导入Spring的配置文件，让配置文件里面的内容生效 Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别； 想让Spring 的配置文件生效，加载进来；需要将@ImportResource标注在一个配置类上 我们写一个beans.xml 并在里面配置bean 123456&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="helloService" class="com.example.demo.serivce.HelloService"&gt;&lt;/bean&gt;&lt;/beans&gt; 接下来我们写测试类,看HelloService是否注册到spring容器中 123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; @Autowired ApplicationContext app; @Test public void testHelloService()&#123; System.out.println(app.containsBean("helloService")); &#125;&#125; 可以看到输出false,所以正如上面所说Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别所以我们再我们的主配置类上增加@ImportResource 注解即可, 12345678@ImportResource(locations = "classpath:beans.xml")@SpringBootApplicationpublic class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; SpringBoot不支持上面的做法，使用@Bean注解进行注册 12345678@Configurationpublic class AppConfig &#123; //方法名即为注册bean id,返回值为实例 @Bean public HelloService helloService()&#123; return new HelloService(); &#125;&#125; 运行测试类,可以看到HelloService 已经注册到我们的IOC容器中了]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot入门]]></title>
    <url>%2F2019%2F09%2F24%2FSpring%20Boot%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、什么是Spring Boot 使用”约定大于配置”的理念让项目快速运行起来 它并不是什么新的框架，而是默认配置了很多框架的使用方式，就像 Maven 整合了所有的 jar 包一样，Spring Boot 整合了所有框架 二、使用Spring Boot有什么好处回顾之前的SSM项目，搭建过程还是比较繁琐的，需要： 配置web.xml,加载过程还是比较繁琐 配置数据库连接、加载spring和spring mvc 配置文件的读取、开启注解 配置mapper文件 …… 使用Spring Boot开发项目简单、快速、方便地搭建项目，极大的提交开发和部署的效率 三、使用IDEA2016快速搭建Spring Boot项目1、配置环境 IDEA2017 jdk1.8 maven 3 SpringBoot 2.1.8 2、file-&gt;new project在弹出的窗口选择Spring Initializr 3、填写项目信息 4、勾上Web模板，mysql连接(如果需要连接数据库) 5、HelloWorld接口简单编写1234567891011121314package com.example.demo;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class HelloController &#123; @RequestMapping("/hello") public String Hello()&#123; return "hello Spring boot"; &#125;&#125; 建立的Controller必须和启动类再同一个包下 6、启动类介绍123456789101112package com.example.demo;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; 7、pom介绍123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; @SpringBootApplication是springBoot的核心注解注解,该注解是@Configuration、@EnableAutoConfiguration、@ComponentScan 注解的合体版 8、启动项目 9、浏览器访问请求 四、 SpringBoot目录介绍 resources中的static中只放置静态页面，如css、js、images等(也可在resources中建立public目录放置静态文件，功能跟static一样) resources中templates中只放置动态页面，动态页面需要先请求服务器，访问后台应用程序，然后再转向页面，比如JSP。Spring Boot建议不要使用JSP，默认使用Thymeleaf来做动态页面，但是如果要使用Thymeleaf，必须添加依赖，如下图。Thymeleaf默认的页面文件后缀是.html。Thymeleaf的语法规范可查看一下链接https://www.e-learn.cn/thymeleaf 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; resources中的application.properties主要配置文件，Spring Boot帮我们免除了大部分的手动配置，但是一些特定的情况，还是需要我们在application中配置，但是只能是被两种格式的配置文件，分别是yml文件和properties文件]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot注解大全]]></title>
    <url>%2F2019%2F09%2F19%2FSpring%20Boot%E6%B3%A8%E8%A7%A3%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[Spring Boot注解大全一、注解列表@SpringBootApplication包含了@ComponentScan、@Configuration和@EnableAutoConfiguration注解。其中@ComponentScan让spring Boot扫描到Configuration类并把它加入到程序上下文。 @ComponentScan 组件扫描，可自动发现和装配一些Bean。 @Configuration 等同于spring的XML配置文件；使用Java代码可以检查类型安全。 @EnableAutoConfiguration 自动配置。 @Component可配合CommandLineRunner使用，在程序启动后执行一些基础任务。 @RestController注解是@Controller和@ResponseBody的合集,表示这是个控制器bean,并且是将函数的返回值直 接填入HTTP响应体中,是REST风格的控制器。 @Autowired自动导入。 @PathVariable获取参数。 123456@RequestMapping(”/testPathVariable/&#123;id&#125;") public String testPathVariable(@PathVariable(“id”) Integer id) &#123; System.out.println(“testPathVariable:”+id); return SUCCESS; &#125; 二、常用注解详情@ResponseBody：表示该方法的返回结果直接写入HTTP response body中，一般在异步获取数据时使用，用于构建RESTful的api。在使用@RequestMapping后，返回值通常解析为跳转路径，加上@Responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。比如异步获取json数据，加上@Responsebody后，会直接返回json数据。该注解一般会配合@RequestMapping一起使用 123451 @RequestMapping("/test”)2 @ResponseBody3 public String test()&#123;4 return”ok”;5 &#125; @Controller：用于定义控制器类，在spring项目中由控制器负责将用户发来的URL请求转发到对应的服务接口（service层），一般这个注解在类中，通常方法需要配合注解@RequestMapping。 1234567891011121314 1 @Controller 2 @RequestMapping(“/demoInfo”) 3 public class DemoController &#123; 4 @Autowired 5 private DemoInfoService demoInfoService; 6 7 @RequestMapping(”/hello“) 8 public String hello(Map&lt;String,Object&gt; map)&#123; 9 System.out.println(“DemoController.hello()”);10 map.put(”hello“,”from TemplateController.helloHtml“);11 //会使用hello.html或者hello.ftl模板进行渲染显示.12 return "/hello“;13 &#125;14 &#125; @RestController：用于标注控制层组件(如struts中的action)，@ResponseBody和@Controller的合集。 123456789101112131415 1 package com.kfit.demo.web; 2 3 import org.springframework.web.bind.annotation.RequestMapping; 4 import org.springframework.web.bind.annotation.RestController; 5 6 7 @RestController 8 @RequestMapping(“/demoInfo2”) 9 publicclass DemoController2 &#123;10 11 @RequestMapping("/test")12 public String test()&#123;13 return "ok";14 &#125;15 &#125; @Import：用来导入其他配置类。 @ImportResource：用来加载xml配置文件。 @Autowired：自动导入依赖的bean @Service：一般用于修饰service层的组件 @Repository：使用@Repository注解可以确保DAO或者repositories提供异常转译，这个注解修饰的DAO或者repositories类会被ComponetScan发现并配置，同时也不需要为它们提供XML配置项。 @Bean：用@Bean标注方法等价于XML中配置的bean。 @Value：注入Spring boot application.properties配置的属性的值。 121 @Value(value = “#&#123;message&#125;”) 2 private String message; @Component：泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 @Bean:相当于XML中的,放在方法的上面，而不是类，意思是产生一个bean,并交给spring管理。 @Qualifier：当有多个同一类型的Bean时，可以用@Qualifier(“name”)来指定。与@Autowired配合使用。@Qualifier限定描述符除了能根据名字进行注入，但能进行更细粒度的控制如何选择候选者 1231 @Autowired2 @Qualifier(value = “demoInfoService”)3 private DemoInfoService demoInfoService;]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Typora编辑器介绍]]></title>
    <url>%2F2019%2F09%2F16%2FTypora%E7%BC%96%E8%BE%91%E5%99%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Typora编辑器介绍Typora 是什么？Typora 是一款支持实时预览的 Markdown 文本编辑器。它有 OS X、Windows、Linux 三个平台的版本，并且由于仍在测试中，是完全免费的，本网站使用的编辑器就是这款，使用简便，用起来很舒服。下载地址https://www.typora.io/ 关于 MarkdownMarkdown 是用来编写结构化文档的一种纯文本格式，它使我们在双手不离开键盘的情况下，可以对文本进行一定程度的格式排版。 Typora快捷键windows快捷键： 无序列表：输入-之后输入空格 有序列表：输入数字+“.”之后输入空格 任务列表：-[空格]空格 文字 标题：ctrl+数字 表格：ctrl+t 生成目录：[TOC]按回车 选中一整行：ctrl+l 选中单词：ctrl+d 选中相同格式的文字：ctrl+e 跳转到文章开头：ctrl+home 跳转到文章结尾：ctrl+end 搜索：ctrl+f 替换：ctrl+h 引用：输入&gt;之后输入空格 代码块：ctrl+alt+f 加粗：ctrl+b 倾斜：ctrl+i 下划线：ctrl+u 删除线：alt+shift+5 插入图片：直接拖动到指定位置即可或者ctrl+shift+i 插入链接：ctrl + k Mac中的快捷键： 最大标题：command + 1 或者：# 大标题：command + 2 或者：## 标准标题：command + 3 或者：### 中标题：command + 4 或者：#### 小标题：command + 5 或者：##### 插入表格：command + T 插入代码：command + alt +c 行间公式 command + Alt + b 段落：command + 0 竖线 ： command + Alt +q 有序列表（1. 2.） ：输入数字+“.”之后输入空格 或者：command + Alt + o 黑点标记：command + Alt + u 隔离线shift + command + - 超链接：command + Alt + l 插入链接：command +k 下划线：command +u 加粗：command +b 搜索：command +f 表情输出表情需要借助 ：符号。 栗子：:smile 显示为 😄,记住是左右两边都要冒号。 使用者可以通过使用ESC键触发表情建议补全功能，也可在功能面板启用后自动触发此功能。同时，直接从菜单栏Edit -&gt; Emoji &amp; Symbols插入UTF8表情符号也是可以的。 或者使用下面的方法 访问网站 https://emojikeyboard.org/,找到需要的符号，鼠标左键单击，然后粘贴到需要的地方就行了！🆗]]></content>
      <categories>
        <category>Typora</category>
      </categories>
      <tags>
        <tag>Typora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJIDEA常用快捷键总结]]></title>
    <url>%2F2019%2F06%2F14%2FIntelliJ%20IDEA%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[IntelliJIDEA常用快捷键总结之前开发项目一直用的是eclipse进行开发，近期在使用IDEA这个工具进行项目开发，之前在eclipse上能使用的快捷键方法放在IDEA上很多都不适用了，因此在此总结一下关于IDEA快捷键的使用方法： 快速查找在文件中的内容并显示：Ctrl+Shift+F 快速跳转到具体文件夹中：按两下Shift 重命名：Shift+F6 切换大小写：Ctrl+Shift+U 产生构造方法，get/set方法等：Alt+Insert 自动提示完成，抛出异常：Alt+Enter 显示所有类或者方法的同名的类型:Ctrl+P 快速打印输出:除了用sout开头快速生成，还可以使用变量名.sout 快速定义局部变量:在字符串或者数字后面输入.var,回车 快速判断（非）空：非空：.notnull或者.nu 空：.null 快速遍历集合：.for, .fori, .forr 都可以满足你的要求。]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap的底层实现和面试问题]]></title>
    <url>%2F2019%2F06%2F01%2FHashMap%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%92%8C%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[HashMap的底层实现和面试问题HashMap简介HashMap数据结构是哈希表结构（数组+链表） HashMap是一个散列表，它存储的内容是键值对（key-value）映射 HashMap继承于AbstractMap，实现了Map、Cloneable、java.io.Serializable接口。 HashMap 的实现不是同步的，这意味着它不是线程安全的。它的key、value都可以为null。此外，HashMap中的映射不是有序的 面试题 LinkedHashMap和hashMap和TreeMap的区别以及应用场景？ 区别 LinkedHashMap是继承于HashMap，是基于HashMap和双向链表来实现的。 HashMap无序；LinkedHashMap有序，可分为插入顺序和访问顺序两种。如果是访问顺序，那put和get操作已存在的Entry时，都会把Entry移动到双向链表的表尾(其实是先删除再插入)。 LinkedHashMap存取数据，还是跟HashMap一样使用的Entry[]的方式，双向链表只是为了保证顺序。 LinkedHashMap是线程不安全的。 TreeMap基于红黑树的实现（主要是排序），默认的排序为升序，如果要改变其排序可以自己写一个Comparator ConcurrentHashMap 线程安全的Map 应用场景 HashMap：在 Map 中插入、删除和定位元素时； TreeMap：在需要按自然顺序或自定义顺序遍历键的情况下 LinkedHashMap：在需要输出的顺序和输入的顺序相同的情况下。 Map简介 Map中键必须是唯一的，值可以重复 面试题 如何遍历map 12345Map&lt;String, String&gt;map = new HashMap&lt;String,String&gt;(); map.put("student1", "阿伟"); map.put("student2", "小李"); map.put("student3", "小张"); map.put("student4", "小王"); 使用entrySet()遍历 1234567Iterator it = map.entrySet().iterator();while (it.hasNext()) &#123; Map.Entry entry =(Map.Entry) it.next(); Object key = entry.getKey(); Object value = entry.getValue(); System.out.println("key="+key+" value"+value); &#125; 通过Map.Keyset遍历key和value,普遍使用，二次取值 12for(String key:map.keySet())&#123; System.out.println("Key="+key+"\tvalue="+map.get(key));&#125; 通过map.values()遍历所有的value，但不能遍历key 123for(String v:map.values())&#123; System.out.println("value="+v);&#125; map.entrySet遍历key和value(推荐使用，特别是容量大时) 123for(Map.Entry&lt;String, String&gt; entry:map.entrySet())&#123; System.out.println("key="+entry.getKey()+"\tvalue="+entry.getValue());&#125; HashMap底层代码实现HashMap的主要参数有哪些？ 桶（capacity）容量，即数组长度：DEFAULT_INITIAL_CAPACITY=1&lt;&lt;4；默认值为16，即在不提供有参构造的时候，声明的hashmap的桶容量； MAXIMUM_CAPACITY = 1 &lt;&lt; 30;极限容量，表示hashmap能承受的最大桶容量为2的30次方，超过这个容量将不再扩容，让hash碰撞起来吧！ static final float DEFAULT_LOAD_FACTOR = 0.75f;负载因子（loadfactor，默认0.75），负载因子有个奇特的效果，表示当当前容量大于（size/）时，将进行hashmap的扩容，扩容一般为扩容为原来的两倍。 int threshold;阈值 阈值算法为capacity*loadfactory，大致当map中entry数量大于此阈值时进行扩容（1.8） transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;（默认为空{}） 核心的数据结构，即所谓的数组+链表的部分。 HashMap的数据结构是什么样子的？ 主要数据结构即为数组+链表。 在hashmap中的主要表现形式为一个table，类型为Entry&lt;K,V&gt;[] table 首先是一个Entry型的数组，Entry为hashmap的内部类： 源码如下 1234567 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash;&#125; 结构如图所示 hash算法hash算法是计算key值对应哈希桶的位置即索引值。我们都知道数组在获取元素会比链表快，所以我们应该尽量让每个哈希桶只有一个元素，这样在查询时就只需要通过索引值找到对应的哈希桶内的值，而不需要再通过桶内的链表一个一个去查。所以hash算法的作用是为了让元素分散均匀，从而提高查询效率。源码如下 12345678910111213141516171819202122232425//这是根据key值获取value值的方法public V get(Object key) &#123; Node&lt;K,V&gt; e; //先调用hash(key) return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;//Hash算法如下static final int hash(Object key) &#123; int h; //第一步：先获取key中的hashCode值 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); //第二步 再与hashcode向左移16位的值进行抑或 &#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; //第三步 n是指数组长度，hash与(n-1)进行&amp;与运算 (first = tab[(n - 1) &amp; hash]) != null) &#123; //省略 ... ... &#125; return null;&#125; 首先第一步获取hashcode没什么问题，到第二步为什么会跟hashcode左移16位的值进行抑或呢? 其实是将高位与低位进行与运算，减少碰撞机率。第三步取余运算，但在计算机运算中&amp;肯定比%快，又因为h % n = h &amp;（n - 1），所以最终将第二步得到的hash跟n-1进行与运算。n是table中的长度。 HashMap的put增加方法流程put方法是重点也是最复杂的操作。需要掌握在什么情况下要扩容（后面会讲一下是如何扩容的），满足什么条件下链表需要转成红黑树。下面是put方法中流程图 HashMap的get方法，查询元素查询get方法相对简单，只要明白hash算法后得到哈希桶的索引值，再对桶内的链表进行比较hash，key是否相等。 扩容扩容是Hashmap重点中的重点。也是最耗性能的操作。扩容的步骤是先对size扩大两倍，再对原先的节点重新经过hash算法得到新的索引值即复制到新的哈希桶里。最后得到新的table。其中jdk8对扩容进行了优化，提高了扩容的效率。但在平常运用中尽量要避免让hashmap进行扩容，若已知hashmap中的元素数量，则一开始初始化hashmap时指定容量，这样就减少了hashmap扩容次数。 面试题 HashMap底层是如何实现的？首先底层数据结构是由数组+链表组成链表散列。HashMap先得到key的散列值，在通过扰动函数（减少碰撞次数）得到Hash值，接着通过hash &amp; （n -1 ），n位table的长度，运算后得到数组的索引值。如果当前节点存在元素，则通过比较hash值和key值是否相等，相等则替换，不相等则通过拉链法查找元素，直到找到相等或者下个节点为null时。1.8对扰动函数，扩容方法进行优化，并且增加了红黑树的数据结构。 HashMap 和 Hashtable 的区别线程安全 HashMap是线程不安全的，而HashTable是线程安全的，每个人方法通过修饰synchronized来控制线程安全。效率 HashMap比HashTable效率高，原因在于HashTable的方法通过synchronized修饰后，并发的效率会降低。允不允许null HashMap运行只有一个key为null，可以有多个null的value。而HashTable不允许key，value为null HashMap的长度为什么是2的倍数在HashMap的操作流程中，首先会对key进行hash算法得到一个索引值，这个索引值就是对应哈希桶数组的索引。为了得到这个索引值必须对扰动后的数跟数组长度进行取余运算。即 hash % n (n为hashmap的长度)，又因为&amp;比%运算快。n如果为2的倍数，就可以将%转换为&amp;，结果就是 hash &amp; (n-1)。所以这就解释了为什么HashMap长度是2的倍数。 Jdk1.8中满足什么条件后将链表转化成红黑树？很显然在putVal方法中是判断桶内的节点个数是否大于8，之后通过treeifyBin方法中判断长度是否大于最小红黑树容量64,小于则继续扩容，大于则转为红黑树。]]></content>
      <categories>
        <category>HashMap</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch介绍和安装]]></title>
    <url>%2F2019%2F06%2F01%2FElasticsearch%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[0.学习目标 独立安装Elasticsearch 会使用Rest的API操作索引 会使用Rest的API查询数据 会使用Rest的API聚合数据 掌握Spring Data Elasticsearch使用 1.Elasticsearch介绍和安装用户访问我们的首页，一般都会直接搜索来寻找自己想要购买的商品。 而商品的数量非常多，而且分类繁杂。如果能正确的显示出用户想要的商品，并进行合理的过滤，尽快促成交易，是搜索系统要研究的核心。 面对这样复杂的搜索业务和数据量，使用传统数据库搜索就显得力不从心，一般我们都会使用全文检索技术，比如之前大家学习过的Solr。 不过今天，我们要讲的是另一个全文检索技术：Elasticsearch。 1.1.简介1.1.1.ElasticElastic官网：https://www.elastic.co/cn/ Elastic有一条完整的产品线及解决方案：Elasticsearch、Kibana、Logstash等，前面说的三个就是大家常说的ELK技术栈。 1.1.2.ElasticsearchElasticsearch官网：https://www.elastic.co/cn/products/elasticsearch 如上所述，Elasticsearch具备以下特点： 分布式，无需人工搭建集群（solr就需要人为配置，使用Zookeeper作为注册中心） Restful风格，一切API都遵循Rest原则，容易上手 近实时搜索，数据更新在Elasticsearch中几乎是完全同步的。 1.1.3.版本目前Elasticsearch最新的版本是6.3.1，我们就使用6.3.0 需要虚拟机JDK1.8及以上 1.2.安装和配置为了模拟真实场景，我们将在linux下安装Elasticsearch。 1.2.1.新建一个用户leyou出于安全考虑，elasticsearch默认不允许以root账号运行。 创建用户： 1useradd leyou 设置密码： 1passwd leyou 切换用户： 1su - leyou 1.2.2.上传安装包,并解压我们将安装包上传到：/home/leyou目录 解压缩： 1tar -zxvf elasticsearch-6.2.4.tar.gz 我们把目录重命名： 1mv elasticsearch-6.2.4/ elasticsearch 进入，查看目录结构： 1.2.3.修改配置我们进入config目录：cd config 需要修改的配置文件有两个： jvm.options Elasticsearch基于Lucene的，而Lucene底层是java实现，因此我们需要配置jvm参数。 编辑jvm.options： 1vim jvm.options 默认配置如下： 12-Xms1g-Xmx1g 内存占用太多了，我们调小一些： 12-Xms512m-Xmx512m elasticsearch.yml 1vim elasticsearch.yml 修改数据和日志目录： 12path.data: /home/leyou/elasticsearch/data # 数据目录位置path.logs: /home/leyou/elasticsearch/logs # 日志目录位置 我们把data和logs目录修改指向了elasticsearch的安装目录。但是这两个目录并不存在，因此我们需要创建出来。 进入elasticsearch的根目录，然后创建： 12mkdir datamkdir logs 修改绑定的ip： 1network.host: 0.0.0.0 # 绑定到0.0.0.0，允许任何ip来访问 默认只允许本机访问，修改为0.0.0.0后则可以远程访问 目前我们是做的单机安装，如果要做集群，只需要在这个配置文件中添加其它节点信息即可。 elasticsearch.yml的其它可配置信息： 属性名 说明 cluster.name 配置elasticsearch的集群名称，默认是elasticsearch。建议修改成一个有意义的名称。 node.name 节点名，es会默认随机指定一个名字，建议指定一个有意义的名称，方便管理 path.conf 设置配置文件的存储路径，tar或zip包安装默认在es根目录下的config文件夹，rpm安装默认在/etc/ elasticsearch path.data 设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开 path.logs 设置日志文件的存储路径，默认是es根目录下的logs文件夹 path.plugins 设置插件的存放路径，默认是es根目录下的plugins文件夹 bootstrap.memory_lock 设置为true可以锁住ES使用的内存，避免内存进行swap network.host 设置bind_host和publish_host，设置为0.0.0.0允许外网访问 http.port 设置对外服务的http端口，默认为9200。 transport.tcp.port 集群结点之间通信端口 discovery.zen.ping.timeout 设置ES自动发现节点连接超时的时间，默认为3秒，如果网络延迟高可设置大些 discovery.zen.minimum_master_nodes 主结点数量的最少值 ,此值的公式为：(master_eligible_nodes / 2) + 1 ，比如：有3个符合要求的主结点，那么这里要设置为2 1.3.运行进入elasticsearch/bin目录，可以看到下面的执行文件： 然后输入命令： 1./elasticsearch 发现报错了，启动失败： 1.3.1.错误1：内核过低 我们使用的是centos6，其linux内核版本为2.6。而Elasticsearch的插件要求至少3.5以上版本。不过没关系，我们禁用这个插件即可。 修改elasticsearch.yml文件，在最下面添加如下配置： 1bootstrap.system_call_filter: false 然后重启 1.3.2.错误2：文件权限不足再次启动，又出错了： 1[1]: max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] 我们用的是leyou用户，而不是root，所以文件权限不足。 首先用root用户登录。 然后修改配置文件: 1vim /etc/security/limits.conf 添加下面的内容： 1234567* soft nofile 65536* hard nofile 131072* soft nproc 4096* hard nproc 4096 1.3.3.错误3：线程数不够刚才报错中，还有一行： 1[1]: max number of threads [1024] for user [leyou] is too low, increase to at least [4096] 这是线程数不够。 继续修改配置： 1vim /etc/security/limits.d/90-nproc.conf 修改下面的内容： 1* soft nproc 1024 改为： 1* soft nproc 4096 1.3.4.错误4：进程虚拟内存1[3]: max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] vm.max_map_count：限制一个进程可以拥有的VMA(虚拟内存区域)的数量，继续修改配置文件， ： 1vim /etc/sysctl.conf 添加下面内容： 1vm.max_map_count=655360 然后执行命令： 1sysctl -p 1.3.5.重启终端窗口所有错误修改完毕，一定要重启你的 Xshell终端，否则配置无效。 1.3.6.启动再次启动，终于成功了！ 可以看到绑定了两个端口: 9300：集群节点间通讯接口 9200：客户端访问接口 我们在浏览器中访问：http://192.168.56.101:9200 1.4.安装kibana1.4.1.什么是Kibana？ Kibana是一个基于Node.js的Elasticsearch索引库数据统计工具，可以利用Elasticsearch的聚合功能，生成各种图表，如柱形图，线状图，饼图等。 而且还提供了操作Elasticsearch索引数据的控制台，并且提供了一定的API提示，非常有利于我们学习Elasticsearch的语法。 1.4.2.安装因为Kibana依赖于node，我们的虚拟机没有安装node，而window中安装过。所以我们选择在window下使用kibana。 最新版本与elasticsearch保持一致，也是6.3.0 解压到特定目录即可 1.4.3.配置运行 配置 进入安装目录下的config目录，修改kibana.yml文件： 修改elasticsearch服务器的地址： 1elasticsearch.url: &quot;http://192.168.56.101:9200&quot; 运行 进入安装目录下的bin目录： 双击运行： 发现kibana的监听端口是5601 我们访问：http://127.0.0.1:5601 1.4.4.控制台选择左侧的DevTools菜单，即可进入控制台页面： 在页面右侧，我们就可以输入请求，访问Elasticsearch了。 1.5.安装ik分词器Lucene的IK分词器早在2012年已经没有维护了，现在我们要使用的是在其基础上维护升级的版本，并且开发为ElasticSearch的集成插件了，与Elasticsearch一起维护升级，版本也保持一致，最新版本：6.3.0 1.5.1.安装上传课前资料中的zip包，解压到Elasticsearch目录的plugins目录中： 使用unzip命令解压： 1unzip elasticsearch-analysis-ik-6.3.0.zip -d ik-analyzer 然后重启elasticsearch： 1.5.2.测试大家先不管语法，我们先测试一波。 在kibana控制台输入下面的请求： 12345POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;我是中国人&quot;&#125; 运行得到结果： 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;我&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;是&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;中国人&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;中国&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;国人&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 4 &#125; ]&#125; 1.7.APIElasticsearch提供了Rest风格的API，即http请求接口，而且也提供了各种语言的客户端API 1.7.1.Rest风格API文档地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 1.7.2.客户端APIElasticsearch支持的客户端非常多：https://www.elastic.co/guide/en/elasticsearch/client/index.html 点击Java Rest Client后，你会发现又有两个： Low Level Rest Client是低级别封装，提供一些基础功能，但更灵活 High Level Rest Client，是在Low Level Rest Client基础上进行的高级别封装，功能更丰富和完善，而且API会变的简单 1.7.3.如何学习建议先学习Rest风格API，了解发起请求的底层实现，请求体格式等。 2.操作索引2.1.基本概念Elasticsearch也是基于Lucene的全文检索库，本质也是存储数据，很多概念与MySQL类似的。 对比关系： 索引（indices）——————————–Databases 数据库 ​ 类型（type）—————————–Table 数据表 ​ 文档（Document）—————-Row 行 ​ 字段（Field）——————-Columns 列 详细说明： 概念 说明 索引库（indices) indices是index的复数，代表许多的索引， 类型（type） 类型是模拟mysql中的table概念，一个索引库下可以有不同类型的索引，比如商品索引，订单索引，其数据格式不同。不过这会导致索引库混乱，因此未来版本中会移除这个概念 文档（document） 存入索引库原始的数据。比如每一条商品信息，就是一个文档 字段（field） 文档中的属性 映射配置（mappings） 字段的数据类型、属性、是否索引、是否存储等特性 是不是与Lucene和solr中的概念类似。 另外，在SolrCloud中，有一些集群相关的概念，在Elasticsearch也有类似的： 索引集（Indices，index的复数）：逻辑上的完整索引 分片（shard）：数据拆分后的各个部分 副本（replica）：每个分片的复制 要注意的是：Elasticsearch本身就是分布式的，因此即便你只有一个节点，Elasticsearch默认也会对你的数据进行分片和副本操作，当你向集群添加新数据时，数据也会在新加入的节点中进行平衡。 2.2.创建索引2.2.1.语法Elasticsearch采用Rest风格API，因此其API就是一次http请求，你可以用任何工具发起http请求 创建索引的请求格式： 请求方式：PUT 请求路径：/索引库名 请求参数：json格式： 123456&#123; "settings": &#123; "number_of_shards": 3, "number_of_replicas": 2 &#125;&#125; settings：索引库的设置 number_of_shards：分片数量 number_of_replicas：副本数量 2.2.2.测试我们先用RestClient来试试 响应： 可以看到索引创建成功了。 2.2.3.使用kibana创建kibana的控制台，可以对http请求进行简化，示例： 相当于是省去了elasticsearch的服务器地址 而且还有语法提示，非常舒服。 2.3.查看索引设置 语法 Get请求可以帮我们查看索引信息，格式： 1GET /索引库名 或者，我们可以使用*来查询所有索引库配置： 2.4.删除索引删除索引使用DELETE请求 语法 1DELETE /索引库名 示例 再次查看heima2： 当然，我们也可以用HEAD请求，查看索引是否存在： 2.5.映射配置索引有了，接下来肯定是添加数据。但是，在添加数据之前必须定义映射。 什么是映射？ ​ 映射是定义文档的过程，文档包含哪些字段，这些字段是否保存，是否索引，是否分词等 只有配置清楚，Elasticsearch才会帮我们进行索引库的创建（不一定） 2.5.1.创建映射字段 语法 请求方式依然是PUT 1234567891011PUT /索引库名/_mapping/类型名称&#123; &quot;properties&quot;: &#123; &quot;字段名&quot;: &#123; &quot;type&quot;: &quot;类型&quot;, &quot;index&quot;: true， &quot;store&quot;: true， &quot;analyzer&quot;: &quot;分词器&quot; &#125; &#125;&#125; 类型名称：就是前面将的type的概念，类似于数据库中的不同表字段名：任意填写 ，可以指定许多属性，例如： type：类型，可以是text、long、short、date、integer、object等 index：是否索引，默认为true store：是否存储，默认为false analyzer：分词器，这里的ik_max_word即使用ik分词器 示例 发起请求： 12345678910111213141516PUT heima/_mapping/goods&#123; "properties": &#123; "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125;, "images": &#123; "type": "keyword", "index": "false" &#125;, "price": &#123; "type": "float" &#125; &#125;&#125; 响应结果： 123&#123; &quot;acknowledged&quot;: true&#125; 2.5.2.查看映射关系 语法： 1GET /索引库名/_mapping 示例： 1GET /heima/_mapping 响应： 123456789101112131415161718192021&#123; "heima": &#123; "mappings": &#123; "goods": &#123; "properties": &#123; "images": &#123; "type": "keyword", "index": false &#125;, "price": &#123; "type": "float" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125; &#125;&#125; 2.5.3.字段属性详解2.5.3.1.typeElasticsearch中支持的数据类型非常丰富： 我们说几个关键的： String类型，又分两种： text：可分词，不可参与聚合 keyword：不可分词，数据会作为完整字段进行匹配，可以参与聚合 Numerical：数值类型，分两类 基本数据类型：long、interger、short、byte、double、float、half_float 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如10或100。elasticsearch会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型 elasticsearch可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为long，节省空间。 2.5.3.2.indexindex影响字段的索引情况。 true：字段会被索引，则可以用来进行搜索。默认值就是true false：字段不会被索引，不能用来搜索 index的默认值就是true，也就是说你不进行任何配置，所有字段都会被索引。 但是有些字段是我们不希望被索引的，比如商品的图片信息，就需要手动设置index为false。 2.5.3.3.store是否将数据进行额外存储。 在学习lucene和solr时，我们知道如果一个字段的store设置为false，那么在文档列表中就不会有这个字段的值，用户的搜索结果中不会显示出来。 但是在Elasticsearch中，即便store设置为false，也可以搜索到结果。 原因是Elasticsearch在创建文档索引时，会将文档中的原始数据备份，保存到一个叫做_source的属性中。而且我们可以通过过滤_source来选择哪些要显示，哪些不显示。 而如果设置store为true，就会在_source以外额外存储一份数据，多余，因此一般我们都会将store设置为false，事实上，store的默认值就是false。 2.5.3.4.boost激励因子，这个与lucene中一样 其它的不再一一讲解，用的不多，大家参考官方文档： 2.6.新增数据2.6.1.随机生成id通过POST请求，可以向一个已经存在的索引库中添加数据。 语法： 1234POST /索引库名/类型名&#123; &quot;key&quot;:&quot;value&quot;&#125; 示例： 123456POST /heima/goods/&#123; "title":"小米手机", "images":"http://image.leyou.com/12479122.jpg", "price":2699.00&#125; 响应： 1234567891011121314&#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_version": 1, "result": "created", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;, "_seq_no": 0, "_primary_term": 2&#125; 通过kibana查看数据： 123456get _search&#123; "query":&#123; "match_all":&#123;&#125; &#125;&#125; 123456789101112&#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_version": 1, "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2699 &#125;&#125; _source：源文档信息，所有的数据都在里面。 _id：这条文档的唯一标示，与文档自己的id字段没有关联 2.6.2.自定义id如果我们想要自己新增的时候指定id，可以这么做： 1234POST /索引库名/类型/id值&#123; ...&#125; 示例： 123456POST /heima/goods/2&#123; "title":"大米手机", "images":"http://image.leyou.com/12479122.jpg", "price":2899.00&#125; 得到的数据： 1234567891011&#123; "_index": "heima", "_type": "goods", "_id": "2", "_score": 1, "_source": &#123; "title": "大米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2899 &#125;&#125; 2.6.3.智能判断在学习Solr时我们发现，我们在新增数据时，只能使用提前配置好映射属性的字段，否则就会报错。 不过在Elasticsearch中并没有这样的规定。 事实上Elasticsearch非常智能，你不需要给索引库设置任何mapping映射，它也可以根据你输入的数据来判断类型，动态添加数据映射。 测试一下： 12345678POST /heima/goods/3&#123; "title":"超米手机", "images":"http://image.leyou.com/12479122.jpg", "price":2899.00, "stock": 200, "saleable":true&#125; 我们额外添加了stock库存，和saleable是否上架两个字段。 来看结果： 1234567891011121314&#123; "_index": "heima", "_type": "goods", "_id": "3", "_version": 1, "_score": 1, "_source": &#123; "title": "超米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2899, "stock": 200, "saleable": true &#125;&#125; 在看下索引库的映射关系: 123456789101112131415161718192021222324252627&#123; "heima": &#123; "mappings": &#123; "goods": &#123; "properties": &#123; "images": &#123; "type": "keyword", "index": false &#125;, "price": &#123; "type": "float" &#125;, "saleable": &#123; "type": "boolean" &#125;, "stock": &#123; "type": "long" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125; &#125;&#125; stock和saleable都被成功映射了。 2.7.修改数据把刚才新增的请求方式改为PUT，就是修改了。不过修改必须指定id， id对应文档存在，则修改 id对应文档不存在，则新增 比如，我们把id为3的数据进行修改： 12345678PUT /heima/goods/3&#123; "title":"超大米手机", "images":"http://image.leyou.com/12479122.jpg", "price":3899.00, "stock": 100, "saleable":true&#125; 结果： 1234567891011121314151617181920212223242526272829&#123; "took": 17, "timed_out": false, "_shards": &#123; "total": 9, "successful": 9, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 1, "_source": &#123; "title": "超大米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 3899, "stock": 100, "saleable": true &#125; &#125; ] &#125;&#125; 2.8.删除数据删除使用DELETE请求，同样，需要根据id进行删除： 语法 1DELETE /索引库名/类型名/id值 示例： 3.查询我们从4块来讲查询： 基本查询 _source过滤 结果过滤 高级查询 排序 3.1.基本查询： 基本语法 12345678GET /索引库名/_search&#123; "query":&#123; "查询类型":&#123; "查询条件":"查询条件值" &#125; &#125;&#125; 这里的query代表一个查询对象，里面可以有不同的查询属性 查询类型： 例如：match_all， match，term ， range 等等 查询条件：查询条件会根据类型的不同，写法也有差异，后面详细讲解 3.1.1 查询所有（match_all) 示例： 123456GET /heima/_search&#123; "query":&#123; "match_all": &#123;&#125; &#125;&#125; query：代表查询对象 match_all：代表查询所有 结果： 1234567891011121314151617181920212223242526272829303132333435363738&#123; "took": 2, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "2", "_score": 1, "_source": &#123; "title": "大米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2899 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2699 &#125; &#125; ] &#125;&#125; took：查询花费时间，单位是毫秒 time_out：是否超时 _shards：分片信息 hits：搜索结果总览对象 total：搜索到的总条数 max_score：所有结果中文档得分的最高分 hits：搜索结果的文档对象数组，每个元素是一条搜索到的文档信息 _index：索引库 _type：文档类型 _id：文档id _score：文档得分 _source：文档的源数据 3.1.2 匹配查询（match）我们先加入一条数据，便于测试： 123456PUT /heima/goods/3&#123; "title":"小米电视4A", "images":"http://image.leyou.com/12479122.jpg", "price":3899.00&#125; 现在，索引库中有2部手机，1台电视： or关系 match类型查询，会把查询条件进行分词，然后进行查询,多个词条之间是or的关系 12345678GET /heima/_search&#123; "query":&#123; "match":&#123; "title":"小米电视" &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728"hits": &#123; "total": 2, "max_score": 0.6931472, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "tmUBomQB_mwm6wH_EC1-", "_score": 0.6931472, "_source": &#123; "title": "小米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2699 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 0.5753642, "_source": &#123; "title": "小米电视4A", "images": "http://image.leyou.com/12479122.jpg", "price": 3899 &#125; &#125; ]&#125; 在上面的案例中，不仅会查询到电视，而且与小米相关的都会查询到，多个词之间是or的关系。 and关系 某些情况下，我们需要更精确查找，我们希望这个关系变成and，可以这样做： 1234567891011GET /heima/_search&#123; "query":&#123; "match": &#123; "title": &#123; "query": "小米电视", "operator": "and" &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took": 2, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 0.5753642, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 0.5753642, "_source": &#123; "title": "小米电视4A", "images": "http://image.leyou.com/12479122.jpg", "price": 3899 &#125; &#125; ] &#125;&#125; 本例中，只有同时包含小米和电视的词条才会被搜索到。 or和and之间？ 在 or 与 and 间二选一有点过于非黑即白。 如果用户给定的条件分词后有 5 个查询词项，想查找只包含其中 4 个词的文档，该如何处理？将 operator 操作符参数设置成 and 只会将此文档排除。 有时候这正是我们期望的，但在全文搜索的大多数应用场景下，我们既想包含那些可能相关的文档，同时又排除那些不太相关的。换句话说，我们想要处于中间某种结果。 match 查询支持 minimum_should_match 最小匹配参数， 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量： 1234567891011GET /heima/_search&#123; "query":&#123; "match":&#123; "title":&#123; "query":"小米曲面电视", "minimum_should_match": "75%" &#125; &#125; &#125;&#125; 本例中，搜索语句可以分为3个词，如果使用and关系，需要同时满足3个词才会被搜索到。这里我们采用最小品牌数：75%，那么也就是说只要匹配到总词条数量的75%即可，这里3*75% 约等于2。所以只要包含2个词条就算满足条件了。 结果： 3.1.3 多字段查询（multi_match）multi_match与match类似，不同的是它可以在多个字段中查询 123456789GET /heima/_search&#123; "query":&#123; "multi_match": &#123; "query": "小米", "fields": [ "title", "subTitle" ] &#125; &#125;&#125; 本例中，我们会在title字段和subtitle字段中查询小米这个词 3.1.4 词条匹配(term)term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些未分词的字符串 12345678GET /heima/_search&#123; "query":&#123; "term":&#123; "price":2699.00 &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took": 2, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2699 &#125; &#125; ] &#125;&#125; 3.1.5 多词条精确匹配(terms)terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件： 12345678GET /heima/_search&#123; "query":&#123; "terms":&#123; "price":[2699.00,2899.00,3899.00] &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; "took": 4, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "2", "_score": 1, "_source": &#123; "title": "大米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2899 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "title": "小米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2699 &#125; &#125;, &#123; "_index": "heima", "_type": "goods", "_id": "3", "_score": 1, "_source": &#123; "title": "小米电视4A", "images": "http://image.leyou.com/12479122.jpg", "price": 3899 &#125; &#125; ] &#125;&#125; 3.2.结果过滤默认情况下，elasticsearch在搜索的结果中，会把文档中保存在_source的所有字段都返回。 如果我们只想获取其中的部分字段，我们可以添加_source的过滤 3.2.1.直接指定字段示例： 123456789GET /heima/_search&#123; "_source": ["title","price"], "query": &#123; "term": &#123; "price": 2699 &#125; &#125;&#125; 返回的结果： 1234567891011121314151617181920212223242526&#123; "took": 12, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "r9c1KGMBIhaxtY5rlRKv", "_score": 1, "_source": &#123; "price": 2699, "title": "小米手机" &#125; &#125; ] &#125;&#125; 3.2.2.指定includes和excludes我们也可以通过： includes：来指定想要显示的字段 excludes：来指定不想要显示的字段 二者都是可选的。 示例： 1234567891011GET /heima/_search&#123; "_source": &#123; "includes":["title","price"] &#125;, "query": &#123; "term": &#123; "price": 2699 &#125; &#125;&#125; 与下面的结果将是一样的： 1234567891011GET /heima/_search&#123; "_source": &#123; "excludes": ["images"] &#125;, "query": &#123; "term": &#123; "price": 2699 &#125; &#125;&#125; 3.3 高级查询3.3.1 布尔组合（bool)bool把各种其它查询通过must（与）、must_not（非）、should（或）的方式进行组合 12345678910GET /heima/_search&#123; "query":&#123; "bool":&#123; "must": &#123; "match": &#123; "title": "大米" &#125;&#125;, "must_not": &#123; "match": &#123; "title": "电视" &#125;&#125;, "should": &#123; "match": &#123; "title": "手机" &#125;&#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took": 10, "timed_out": false, "_shards": &#123; "total": 3, "successful": 3, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 0.5753642, "hits": [ &#123; "_index": "heima", "_type": "goods", "_id": "2", "_score": 0.5753642, "_source": &#123; "title": "大米手机", "images": "http://image.leyou.com/12479122.jpg", "price": 2899 &#125; &#125; ] &#125;&#125; 3.3.2 范围查询(range)range 查询找出那些落在指定区间内的数字或者时间 1234567891011GET /heima/_search&#123; "query":&#123; "range": &#123; "price": &#123; "gte": 1000.0, "lt": 2800.00 &#125; &#125; &#125;&#125; range查询允许以下字符： 操作符 说明 gt 大于 gte 大于等于 lt 小于 lte 小于等于 3.3.3 模糊查询(fuzzy)我们新增一个商品： 123456POST /heima/goods/4&#123; "title":"apple手机", "images":"http://image.leyou.com/12479122.jpg", "price":6899.00&#125; fuzzy 查询是 term 查询的模糊等价。它允许用户搜索词条与实际词条的拼写出现偏差，但是偏差的编辑距离不得超过2： 12345678GET /heima/_search&#123; "query": &#123; "fuzzy": &#123; "title": "appla" &#125; &#125;&#125; 上面的查询，也能查询到apple手机 我们可以通过fuzziness来指定允许的编辑距离： 1234567891011GET /heima/_search&#123; "query": &#123; "fuzzy": &#123; "title": &#123; "value":"appla", "fuzziness":1 &#125; &#125; &#125;&#125; 3.4 过滤(filter) 条件查询中进行过滤 所有的查询都会影响到文档的评分及排名。如果我们需要在查询结果中进行过滤，并且不希望过滤条件影响评分，那么就不要把过滤条件作为查询条件来用。而是使用filter方式： 1234567891011GET /heima/_search&#123; "query":&#123; "bool":&#123; "must":&#123; "match": &#123; "title": "小米手机" &#125;&#125;, "filter":&#123; "range":&#123;"price":&#123;"gt":2000.00,"lt":3800.00&#125;&#125; &#125; &#125; &#125;&#125; 注意：filter中还可以再次进行bool组合条件过滤。 无查询条件，直接过滤 如果一次查询只有过滤，没有查询条件，不希望进行评分，我们可以使用constant_score取代只有 filter 语句的 bool 查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。 123456789GET /heima/_search&#123; "query":&#123; "constant_score": &#123; "filter": &#123; "range":&#123;"price":&#123;"gt":2000.00,"lt":3000.00&#125;&#125; &#125; &#125;&#125; 3.5 排序3.4.1 单字段排序sort 可以让我们按照不同的字段进行排序，并且通过order指定排序的方式 123456789101112131415GET /heima/_search&#123; "query": &#123; "match": &#123; "title": "小米手机" &#125; &#125;, "sort": [ &#123; "price": &#123; "order": "desc" &#125; &#125; ]&#125; 3.4.2 多字段排序假定我们想要结合使用 price和 _score（得分） 进行查询，并且匹配的结果首先按照价格排序，然后按照相关性得分排序： 123456789101112131415GET /goods/_search&#123; "query":&#123; "bool":&#123; "must":&#123; "match": &#123; "title": "小米手机" &#125;&#125;, "filter":&#123; "range":&#123;"price":&#123;"gt":200000,"lt":300000&#125;&#125; &#125; &#125; &#125;, "sort": [ &#123; "price": &#123; "order": "desc" &#125;&#125;, &#123; "_score": &#123; "order": "desc" &#125;&#125; ]&#125; 4. 聚合aggregations聚合可以让我们极其方便的实现对数据的统计、分析。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？ 实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现实时搜索效果。 4.1 基本概念Elasticsearch中的聚合，包含多种类型，最常用的两种，一个叫桶，一个叫度量： 桶（bucket） 桶的作用，是按照某种方式对数据进行分组，每一组数据在ES中称为一个桶，例如我们根据国籍对人划分，可以得到中国桶、英国桶，日本桶……或者我们按照年龄段对人进行划分：0~10,10~20,20~30,30~40等。 Elasticsearch中提供的划分桶的方式有很多： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一组 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一组 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分组 …… 综上所述，我们发现bucket aggregations 只负责对数据进行分组，并不进行计算，因此往往bucket中往往会嵌套另一种聚合：metrics aggregations即度量 度量（metrics） 分组完成以后，我们一般会对组中的数据进行聚合运算，例如求平均值、最大、最小、求和等，这些在ES中称为度量 比较常用的一些度量聚合方式： Avg Aggregation：求平均值 Max Aggregation：求最大值 Min Aggregation：求最小值 Percentiles Aggregation：求百分比 Stats Aggregation：同时返回avg、max、min、sum、count等 Sum Aggregation：求和 Top hits Aggregation：求前几 Value Count Aggregation：求总数 …… 为了测试聚合，我们先批量导入一些数据 创建索引： 12345678910111213141516171819PUT /cars&#123; "settings": &#123; "number_of_shards": 1, "number_of_replicas": 0 &#125;, "mappings": &#123; "transactions": &#123; "properties": &#123; "color": &#123; "type": "keyword" &#125;, "make": &#123; "type": "keyword" &#125; &#125; &#125; &#125;&#125; 注意：在ES中，需要进行聚合、排序、过滤的字段其处理方式比较特殊，因此不能被分词。这里我们将color和make这两个文字类型的字段设置为keyword类型，这个类型不会被分词，将来就可以参与聚合 导入数据 1234567891011121314151617POST /cars/transactions/_bulk&#123; "index": &#123;&#125;&#125;&#123; "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 30000, "color" : "green", "make" : "ford", "sold" : "2014-05-18" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 15000, "color" : "blue", "make" : "toyota", "sold" : "2014-07-02" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 12000, "color" : "green", "make" : "toyota", "sold" : "2014-08-19" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 80000, "color" : "red", "make" : "bmw", "sold" : "2014-01-01" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 25000, "color" : "blue", "make" : "ford", "sold" : "2014-02-12" &#125; 4.2 聚合为桶首先，我们按照 汽车的颜色color来划分桶 1234567891011GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125; &#125; &#125;&#125; size： 查询条数，这里设置为0，因为我们不关心搜索到的数据，只关心聚合结果，提高效率 aggs：声明这是一个聚合查询，是aggregations的缩写 popular_colors：给这次聚合起一个名字，任意。 terms：划分桶的方式，这里是根据词条划分 field：划分桶的字段 结果： 1234567891011121314151617181920212223242526272829303132333435&#123; "took": 1, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 8, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "popular_colors": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "red", "doc_count": 4 &#125;, &#123; "key": "blue", "doc_count": 2 &#125;, &#123; "key": "green", "doc_count": 2 &#125; ] &#125; &#125;&#125; hits：查询结果为空，因为我们设置了size为0 aggregations：聚合的结果 popular_colors：我们定义的聚合名称 buckets：查找到的桶，每个不同的color字段值都会形成一个桶 key：这个桶对应的color字段的值 doc_count：这个桶中的文档数量 通过聚合的结果我们发现，目前红色的小车比较畅销！ 4.3 桶内度量前面的例子告诉我们每个桶里面的文档数量，这很有用。 但通常，我们的应用需要提供更复杂的文档度量。 例如，每种颜色汽车的平均价格是多少？ 因此，我们需要告诉Elasticsearch使用哪个字段，使用何种度量方式进行运算，这些信息要嵌套在桶内，度量的运算会基于桶内的文档进行 现在，我们为刚刚的聚合结果添加 求价格平均值的度量： 123456789101112131415161718GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125;, "aggs":&#123; "avg_price": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125;&#125; aggs：我们在上一个aggs(popular_colors)中添加新的aggs。可见度量也是一个聚合,度量是在桶内的聚合 avg_price：聚合的名称 avg：度量的类型，这里是求平均值 field：度量运算的字段 结果： 12345678910111213141516171819202122232425262728293031... "aggregations": &#123; "popular_colors": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "red", "doc_count": 4, "avg_price": &#123; "value": 32500 &#125; &#125;, &#123; "key": "blue", "doc_count": 2, "avg_price": &#123; "value": 20000 &#125; &#125;, &#123; "key": "green", "doc_count": 2, "avg_price": &#123; "value": 21000 &#125; &#125; ] &#125; &#125;... 可以看到每个桶中都有自己的avg_price字段，这是度量聚合的结果 4.4 桶内嵌套桶刚刚的案例中，我们在桶内嵌套度量运算。事实上桶不仅可以嵌套运算， 还可以再嵌套其它桶。也就是说在每个分组中，再分更多组。 比如：我们想统计每种颜色的汽车中，分别属于哪个制造商，按照make字段再进行分桶 1234567891011121314151617181920212223GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125;, "aggs":&#123; "avg_price": &#123; "avg": &#123; "field": "price" &#125; &#125;, "maker":&#123; "terms":&#123; "field":"make" &#125; &#125; &#125; &#125; &#125;&#125; 原来的color桶和avg计算我们不变 maker：在嵌套的aggs下新添一个桶，叫做maker terms：桶的划分类型依然是词条 filed：这里根据make字段进行划分 部分结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374...&#123;"aggregations": &#123; "popular_colors": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "red", "doc_count": 4, "maker": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "honda", "doc_count": 3 &#125;, &#123; "key": "bmw", "doc_count": 1 &#125; ] &#125;, "avg_price": &#123; "value": 32500 &#125; &#125;, &#123; "key": "blue", "doc_count": 2, "maker": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "ford", "doc_count": 1 &#125;, &#123; "key": "toyota", "doc_count": 1 &#125; ] &#125;, "avg_price": &#123; "value": 20000 &#125; &#125;, &#123; "key": "green", "doc_count": 2, "maker": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "ford", "doc_count": 1 &#125;, &#123; "key": "toyota", "doc_count": 1 &#125; ] &#125;, "avg_price": &#123; "value": 21000 &#125; &#125; ] &#125; &#125;&#125;... 我们可以看到，新的聚合maker被嵌套在原来每一个color的桶中。 每个颜色下面都根据 make字段进行了分组 我们能读取到的信息： 红色车共有4辆 红色车的平均售价是 $32，500 美元。 其中3辆是 Honda 本田制造，1辆是 BMW 宝马制造。 4.5.划分桶的其它方式前面讲了，划分桶的方式有很多，例如： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一组 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一组 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分组 刚刚的案例中，我们采用的是Terms Aggregation，即根据词条划分桶。 接下来，我们再学习几个比较实用的： 4.5.1.阶梯分桶Histogram 原理： histogram是把数值类型的字段，按照一定的阶梯大小进行分组。你需要指定一个阶梯值（interval）来划分阶梯大小。 举例： 比如你有价格字段，如果你设定interval的值为200，那么阶梯就会是这样的： 0，200，400，600，… 上面列出的是每个阶梯的key，也是区间的启点。 如果一件商品的价格是450，会落入哪个阶梯区间呢？计算公式如下： 1bucket_key = Math.floor((value - offset) / interval) * interval + offset value：就是当前数据的值，本例中是450 offset：起始偏移量，默认为0 interval：阶梯间隔，比如200 因此你得到的key = Math.floor((450 - 0) / 200) * 200 + 0 = 400 操作一下： 比如，我们对汽车的价格进行分组，指定间隔interval为5000： 123456789101112GET /cars/_search&#123; "size":0, "aggs":&#123; "price":&#123; "histogram": &#123; "field": "price", "interval": 5000 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&#123; "took": 21, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 8, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "price": &#123; "buckets": [ &#123; "key": 10000, "doc_count": 2 &#125;, &#123; "key": 15000, "doc_count": 1 &#125;, &#123; "key": 20000, "doc_count": 2 &#125;, &#123; "key": 25000, "doc_count": 1 &#125;, &#123; "key": 30000, "doc_count": 1 &#125;, &#123; "key": 35000, "doc_count": 0 &#125;, &#123; "key": 40000, "doc_count": 0 &#125;, &#123; "key": 45000, "doc_count": 0 &#125;, &#123; "key": 50000, "doc_count": 0 &#125;, &#123; "key": 55000, "doc_count": 0 &#125;, &#123; "key": 60000, "doc_count": 0 &#125;, &#123; "key": 65000, "doc_count": 0 &#125;, &#123; "key": 70000, "doc_count": 0 &#125;, &#123; "key": 75000, "doc_count": 0 &#125;, &#123; "key": 80000, "doc_count": 1 &#125; ] &#125; &#125;&#125; 你会发现，中间有大量的文档数量为0 的桶，看起来很丑。 我们可以增加一个参数min_doc_count为1，来约束最少文档数量为1，这样文档数量为0的桶会被过滤 示例： 12345678910111213GET /cars/_search&#123; "size":0, "aggs":&#123; "price":&#123; "histogram": &#123; "field": "price", "interval": 5000, "min_doc_count": 1 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; "took": 15, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 8, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "price": &#123; "buckets": [ &#123; "key": 10000, "doc_count": 2 &#125;, &#123; "key": 15000, "doc_count": 1 &#125;, &#123; "key": 20000, "doc_count": 2 &#125;, &#123; "key": 25000, "doc_count": 1 &#125;, &#123; "key": 30000, "doc_count": 1 &#125;, &#123; "key": 80000, "doc_count": 1 &#125; ] &#125; &#125;&#125; 完美，！ 如果你用kibana将结果变为柱形图，会更好看： 4.5.2.范围分桶range范围分桶与阶梯分桶类似，也是把数字按照阶段进行分组，只不过range方式需要你自己指定每一组的起始和结束大小。 5.Spring Data ElasticsearchElasticsearch提供的Java客户端有一些不太方便的地方： 很多地方需要拼接Json字符串，在java中拼接字符串有多恐怖你应该懂的 需要自己把对象序列化为json存储 查询到结果也需要自己反序列化为对象 因此，我们这里就不讲解原生的Elasticsearch客户端API了。 而是学习Spring提供的套件：Spring Data Elasticsearch。 5.1.简介Spring Data Elasticsearch是Spring Data项目下的一个子模块。 查看 Spring Data的官网：http://projects.spring.io/spring-data/ Spring Data的使命是为数据访问提供熟悉且一致的基于Spring的编程模型，同时仍保留底层数据存储的特殊特性。 它使得使用数据访问技术，关系数据库和非关系数据库，map-reduce框架和基于云的数据服务变得容易。这是一个总括项目，其中包含许多特定于给定数据库的子项目。这些令人兴奋的技术项目背后，是由许多公司和开发人员合作开发的。 Spring Data 的使命是给各种数据访问提供统一的编程接口，不管是关系型数据库（如MySQL），还是非关系数据库（如Redis），或者类似Elasticsearch这样的索引数据库。从而简化开发人员的代码，提高开发效率。 包含很多不同数据操作的模块： Spring Data Elasticsearch的页面：https://projects.spring.io/spring-data-elasticsearch/ 特征： 支持Spring的基于@Configuration的java配置方式，或者XML配置方式 提供了用于操作ES的便捷工具类ElasticsearchTemplate。包括实现文档到POJO之间的自动智能映射。 利用Spring的数据转换服务实现的功能丰富的对象映射 基于注解的元数据映射方式，而且可扩展以支持更多不同的数据格式 根据持久层接口自动生成对应实现方法，无需人工编写基本操作代码（类似mybatis，根据接口自动得到实现）。当然，也支持人工定制查询 5.2.创建Demo工程我们新建一个demo，学习Elasticsearch pom依赖： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.leyou.demo&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;elasticsearch&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml文件配置： 12345spring: data: elasticsearch: cluster-name: elasticsearch cluster-nodes: 192.168.56.101:9300 5.3.实体类及注解首先我们准备好实体类： 12345678public class Item &#123; Long id; String title; //标题 String category;// 分类 String brand; // 品牌 Double price; // 价格 String images; // 图片地址&#125; 映射 Spring Data通过注解来声明字段的映射属性，有下面的三个注解： @Document 作用在类，标记实体类为文档对象，一般有两个属性 indexName：对应索引库名称 type：对应在索引库中的类型 shards：分片数量，默认5 replicas：副本数量，默认1 @Id 作用在成员变量，标记一个字段作为id主键 @Field 作用在成员变量，标记为文档的字段，并指定字段映射属性： type：字段类型，取值是枚举：FieldType index：是否索引，布尔类型，默认是true store：是否存储，布尔类型，默认是false analyzer：分词器名称 示例： 1234567891011121314151617181920@Document(indexName = "item",type = "docs", shards = 1, replicas = 0)public class Item &#123; @Id private Long id; @Field(type = FieldType.Text, analyzer = "ik_max_word") private String title; //标题 @Field(type = FieldType.Keyword) private String category;// 分类 @Field(type = FieldType.Keyword) private String brand; // 品牌 @Field(type = FieldType.Double) private Double price; // 价格 @Field(index = false, type = FieldType.Keyword) private String images; // 图片地址&#125; 5.4.Template索引操作5.4.1.创建索引和映射 创建索引 ElasticsearchTemplate中提供了创建索引的API： 可以根据类的信息自动生成，也可以手动指定indexName和Settings 映射 映射相关的API： 可以根据类的字节码信息（注解配置）来生成映射，或者手动编写映射 我们这里采用类的字节码信息创建索引并映射： 123456789101112131415@RunWith(SpringRunner.class)@SpringBootTest(classes = ItcastElasticsearchApplication.class)public class IndexTest &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; @Test public void testCreate()&#123; // 创建索引，会根据Item类的@Document注解信息来创建 elasticsearchTemplate.createIndex(Item.class); // 配置映射，会根据Item类中的id、Field等字段来自动完成映射 elasticsearchTemplate.putMapping(Item.class); &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445GET /item&#123; "item": &#123; "aliases": &#123;&#125;, "mappings": &#123; "docs": &#123; "properties": &#123; "brand": &#123; "type": "keyword" &#125;, "category": &#123; "type": "keyword" &#125;, "images": &#123; "type": "keyword", "index": false &#125;, "price": &#123; "type": "double" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125;, "settings": &#123; "index": &#123; "refresh_interval": "1s", "number_of_shards": "1", "provided_name": "item", "creation_date": "1525405022589", "store": &#123; "type": "fs" &#125;, "number_of_replicas": "0", "uuid": "4sE9SAw3Sqq1aAPz5F6OEg", "version": &#123; "created": "6020499" &#125; &#125; &#125; &#125;&#125; 5.3.2.删除索引删除索引的API： 可以根据类名或索引名删除。 示例： 1234@Testpublic void deleteIndex() &#123; esTemplate.deleteIndex("heima");&#125; 结果： 5.4.Repository文档操作Spring Data 的强大之处，就在于你不用写任何DAO处理，自动根据方法名或类的信息进行CRUD操作。只要你定义一个接口，然后继承Repository提供的一些子接口，就能具备各种基本的CRUD功能。 我们只需要定义接口，然后继承它就OK了。 12public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123;&#125; 来看下Repository的继承关系： 我们看到有一个ElasticsearchRepository接口： 5.4.1.新增文档123456789@Autowiredprivate ItemRepository itemRepository;@Testpublic void index() &#123; Item item = new Item(1L, "小米手机7", " 手机", "小米", 3499.00, "http://image.leyou.com/13123.jpg"); itemRepository.save(item);&#125; 去页面查询看看： 1GET /item/_search 结果： 123456789101112131415161718192021222324252627282930&#123; "took": 14, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "item", "_type": "docs", "_id": "1", "_score": 1, "_source": &#123; "id": 1, "title": "小米手机7", "category": " 手机", "brand": "小米", "price": 3499, "images": "http://image.leyou.com/13123.jpg" &#125; &#125; ] &#125;&#125; 5.4.2.批量新增代码： 12345678@Testpublic void indexList() &#123; List&lt;Item&gt; list = new ArrayList&lt;&gt;(); list.add(new Item(2L, "坚果手机R1", " 手机", "锤子", 3699.00, "http://image.leyou.com/123.jpg")); list.add(new Item(3L, "华为META10", " 手机", "华为", 4499.00, "http://image.leyou.com/3.jpg")); // 接收对象集合，实现批量新增 itemRepository.saveAll(list);&#125; 再次去页面查询： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&#123; "took": 5, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "item", "_type": "docs", "_id": "2", "_score": 1, "_source": &#123; "id": 2, "title": "坚果手机R1", "category": " 手机", "brand": "锤子", "price": 3699, "images": "http://image.leyou.com/13123.jpg" &#125; &#125;, &#123; "_index": "item", "_type": "docs", "_id": "3", "_score": 1, "_source": &#123; "id": 3, "title": "华为META10", "category": " 手机", "brand": "华为", "price": 4499, "images": "http://image.leyou.com/13123.jpg" &#125; &#125;, &#123; "_index": "item", "_type": "docs", "_id": "1", "_score": 1, "_source": &#123; "id": 1, "title": "小米手机7", "category": " 手机", "brand": "小米", "price": 3499, "images": "http://image.leyou.com/13123.jpg" &#125; &#125; ] &#125;&#125; 5.4.3.修改文档修改和新增是同一个接口，区分的依据就是id，这一点跟我们在页面发起PUT请求是类似的。 5.4.4.基本查询ElasticsearchRepository提供了一些基本的查询方法： 我们来试试查询所有： 123456@Testpublic void testFind()&#123; // 查询全部，并安装价格降序排序 Iterable&lt;Item&gt; items = this.itemRepository.findAll(Sort.by(Sort.Direction.DESC, "price")); items.forEach(item-&gt; System.out.println(item));&#125; 结果： 5.4.5.自定义方法Spring Data 的另一个强大功能，是根据方法名称自动实现功能。 比如：你的方法名叫做：findByTitle，那么它就知道你是根据title查询，然后自动帮你完成，无需写实现类。 当然，方法名称要符合一定的约定： Keyword Sample Elasticsearch Query String And findByNameAndPrice {&quot;bool&quot; : {&quot;must&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Or findByNameOrPrice {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Is findByName {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Not findByNameNot {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Between findByPriceBetween {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} LessThanEqual findByPriceLessThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} GreaterThanEqual findByPriceGreaterThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Before findByPriceBefore {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} After findByPriceAfter {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Like findByNameLike {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} StartingWith findByNameStartingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} EndingWith findByNameEndingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;*?&quot;,&quot;analyze_wildcard&quot; : true}}}}} Contains/Containing findByNameContaining {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;**?**&quot;,&quot;analyze_wildcard&quot; : true}}}}} In findByNameIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must&quot; : {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}} ]}}}} NotIn findByNameNotIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;bool&quot; : {&quot;should&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}}}} Near findByStoreNear Not Supported Yet ! True findByAvailableTrue {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} False findByAvailableFalse {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : false}}}} OrderBy findByAvailableTrueOrderByNameDesc {&quot;sort&quot; : [{ &quot;name&quot; : {&quot;order&quot; : &quot;desc&quot;} }],&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} 例如，我们来按照价格区间查询，定义这样的一个方法： 12345678910public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123; /** * 根据价格区间查询 * @param price1 * @param price2 * @return */ List&lt;Item&gt; findByPriceBetween(double price1, double price2);&#125; 然后添加一些测试数据： 1234567891011@Testpublic void indexList() &#123; List&lt;Item&gt; list = new ArrayList&lt;&gt;(); list.add(new Item(1L, "小米手机7", "手机", "小米", 3299.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(2L, "坚果手机R1", "手机", "锤子", 3699.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(3L, "华为META10", "手机", "华为", 4499.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(4L, "小米Mix2S", "手机", "小米", 4299.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(5L, "荣耀V10", "手机", "华为", 2799.00, "http://image.leyou.com/13123.jpg")); // 接收对象集合，实现批量新增 itemRepository.saveAll(list);&#125; 不需要写实现类，然后我们直接去运行： 1234567@Testpublic void queryByPriceBetween()&#123; List&lt;Item&gt; list = this.itemRepository.findByPriceBetween(2000.00, 3500.00); for (Item item : list) &#123; System.out.println("item = " + item); &#125;&#125; 结果： 虽然基本查询和自定义方法已经很强大了，但是如果是复杂查询（模糊、通配符、词条查询等）就显得力不从心了。此时，我们只能使用原生查询。 5.5.高级查询5.5.1.基本查询先看看基本玩法 12345678@Testpublic void testQuery()&#123; // 词条查询 MatchQueryBuilder queryBuilder = QueryBuilders.matchQuery("title", "小米"); // 执行查询 Iterable&lt;Item&gt; items = this.itemRepository.search(queryBuilder); items.forEach(System.out::println);&#125; Repository的search方法需要QueryBuilder参数，elasticSearch为我们提供了一个对象QueryBuilders： QueryBuilders提供了大量的静态方法，用于生成各种不同类型的查询对象，例如：词条、模糊、通配符等QueryBuilder对象。 结果： elasticsearch提供很多可用的查询方式，但是不够灵活。如果想玩过滤或者聚合查询等就很难了。 5.5.2.自定义查询先来看最基本的match query： 1234567891011121314@Testpublic void testNativeQuery()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.matchQuery("title", "小米")); // 执行搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 打印总条数 System.out.println(items.getTotalElements()); // 打印总页数 System.out.println(items.getTotalPages()); items.forEach(System.out::println);&#125; NativeSearchQueryBuilder：Spring提供的一个查询条件构建器，帮助构建json格式的请求体 Page&lt;item&gt;：默认是分页查询，因此返回的是一个分页的结果对象，包含属性： totalElements：总条数 totalPages：总页数 Iterator：迭代器，本身实现了Iterator接口，因此可直接迭代得到当前页的数据 其它属性： 结果： 5.5.4.分页查询利用NativeSearchQueryBuilder可以方便的实现分页： 12345678910111213141516171819202122232425@Testpublic void testNativeQuery()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.termQuery("category", "手机")); // 初始化分页参数 int page = 0; int size = 3; // 设置分页参数 queryBuilder.withPageable(PageRequest.of(page, size)); // 执行搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 打印总条数 System.out.println(items.getTotalElements()); // 打印总页数 System.out.println(items.getTotalPages()); // 每页大小 System.out.println(items.getSize()); // 当前页 System.out.println(items.getNumber()); items.forEach(System.out::println);&#125; 结果： 可以发现，Elasticsearch中的分页是从第0页开始。 5.5.5.排序排序也通用通过NativeSearchQueryBuilder完成： 12345678910111213141516@Testpublic void testSort()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.termQuery("category", "手机")); // 排序 queryBuilder.withSort(SortBuilders.fieldSort("price").order(SortOrder.DESC)); // 执行搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 打印总条数 System.out.println(items.getTotalElements()); items.forEach(System.out::println);&#125; 结果： 5.6.聚合5.6.1.聚合为桶桶就是分组，比如这里我们按照品牌brand进行分组： 12345678910111213141516171819202122232425@Testpublic void testAgg()&#123; NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 不查询任何结果 queryBuilder.withSourceFilter(new FetchSourceFilter(new String[]&#123;""&#125;, null)); // 1、添加一个新的聚合，聚合类型为terms，聚合名称为brands，聚合字段为brand queryBuilder.addAggregation( AggregationBuilders.terms("brands").field("brand")); // 2、查询,需要把结果强转为AggregatedPage类型 AggregatedPage&lt;Item&gt; aggPage = (AggregatedPage&lt;Item&gt;) this.itemRepository.search(queryBuilder.build()); // 3、解析 // 3.1、从结果中取出名为brands的那个聚合， // 因为是利用String类型字段来进行的term聚合，所以结果要强转为StringTerm类型 StringTerms agg = (StringTerms) aggPage.getAggregation("brands"); // 3.2、获取桶 List&lt;StringTerms.Bucket&gt; buckets = agg.getBuckets(); // 3.3、遍历 for (StringTerms.Bucket bucket : buckets) &#123; // 3.4、获取桶中的key，即品牌名称 System.out.println(bucket.getKeyAsString()); // 3.5、获取桶中的文档数量 System.out.println(bucket.getDocCount()); &#125;&#125; 显示的结果： 关键API： AggregationBuilders：聚合的构建工厂类。所有聚合都由这个类来构建，看看他的静态方法： AggregatedPage：聚合查询的结果类。它是Page&lt;T&gt;的子接口： AggregatedPage在Page功能的基础上，拓展了与聚合相关的功能，它其实就是对聚合结果的一种封装，大家可以对照聚合结果的JSON结构来看。 而返回的结果都是Aggregation类型对象，不过根据字段类型不同，又有不同的子类表示 我们看下页面的查询的JSON结果与Java类的对照关系： 5.6.2.嵌套聚合，求平均值代码： 1234567891011121314151617181920212223242526272829@Testpublic void testSubAgg()&#123; NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 不查询任何结果 queryBuilder.withSourceFilter(new FetchSourceFilter(new String[]&#123;""&#125;, null)); // 1、添加一个新的聚合，聚合类型为terms，聚合名称为brands，聚合字段为brand queryBuilder.addAggregation( AggregationBuilders.terms("brands").field("brand") .subAggregation(AggregationBuilders.avg("priceAvg").field("price")) // 在品牌聚合桶内进行嵌套聚合，求平均值 ); // 2、查询,需要把结果强转为AggregatedPage类型 AggregatedPage&lt;Item&gt; aggPage = (AggregatedPage&lt;Item&gt;) this.itemRepository.search(queryBuilder.build()); // 3、解析 // 3.1、从结果中取出名为brands的那个聚合， // 因为是利用String类型字段来进行的term聚合，所以结果要强转为StringTerm类型 StringTerms agg = (StringTerms) aggPage.getAggregation("brands"); // 3.2、获取桶 List&lt;StringTerms.Bucket&gt; buckets = agg.getBuckets(); // 3.3、遍历 for (StringTerms.Bucket bucket : buckets) &#123; // 3.4、获取桶中的key，即品牌名称 3.5、获取桶中的文档数量 System.out.println(bucket.getKeyAsString() + "，共" + bucket.getDocCount() + "台"); // 3.6.获取子聚合结果： InternalAvg avg = (InternalAvg) bucket.getAggregations().asMap().get("priceAvg"); System.out.println("平均售价：" + avg.getValue()); &#125;&#125; 结果：]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化]]></title>
    <url>%2F2019%2F03%2F27%2Fmysql%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言为什么要优化？1、系统的吞吐量瓶颈往往出现在数据库的访问速度上2、随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢3、数据是存放在磁盘上的，读写速度无法和内存相比 如何优化1、设计数据库时：数据库表、字段的设计，存储引擎2、利用好MySQL自身提供的功能，如索引等3、横向扩展：MySQL集群、负载均衡、读写分离4、SQL语句的优化（文章主要讲解） 一、存储引擎的选择1、早期问题：如何选择MyISAM和Innodb？现在不存在这个问题了，Innodb不断完善，从各个方面赶超MyISAM，也是MySQL默认使用的。存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现 2、存储差异 3、选择依据MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键保证数据完整性。比如OA自动化办公系统。 二、索引的使用1、什么是索引？（1）、索引是帮助mysql高效 获取数据的数据结构（排好序的快速查找数据结构）（2）、索引到时指B树（多路搜索树，不一定是二叉的）结构组织的索引 2、如何使用explain来查看是否使用索引（1）、 explain关键字的使用方法很简单，就是把它放在select查询语句的前面，如下图所示：（2）、解释下上图中各字段的意义①id:id值越大，优先级越高，越先执行（一般有子查询的id值越大）；id②值如果相同，从上到下顺序执行（表的读取顺序）②select_type:查询的类型，主要区别普通查询、联合查询、子查询等复杂查询（这个值不需要重点看）③table:显示的查询表名，如果查询使用了别名（这个值不需要重点看）④type:从最好到最差的依次是system&gt;const&gt;eq_ref&gt;range&gt;index&gt;ALL,除了ALL没有使用到索引之外，其他都使用索引，具体介绍如下图⑤possible_keys:查询可能使用到的索引都会在这里列出来⑥key:查询真正使用到的索引，select_type为index_merge时，这里可能出现两个以上的索引，其他的select_type这里只会出现一个。如果为null，则没有使用索引⑦rows:根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数（越小越好） 三、SQL语句的优化1、对进行对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。2、select*会进行全表扫描，要求select后面直接接上字段名3、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描select id from employee where num is null(错误)select id from t where num=0（正确） 4、应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。5、尽量少用or，使用union all（包括重复，不进行排序）或者是union（自动去除重复的值，默认规则的排序）代替or6、能用between就不要用in、not in7、不建议使用%前缀模糊查询，如like“%name”，“%name%”，但是可以使用like“name%”，这种查询会导致索引失效而进行全表扫描，mysql使用全文索引可解决这个问题8、应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描select id from t where num/2=100（错误）select id from t where num=100*2（正确） 9、应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描select id from t where substring(name,1,3)=’abc’–name以abc开头的id（错误）select id from t where name like ‘abc%’（正确） 10、用 exists 代替 in 是一个好的选择select num from a where num in(select num from b)（错误）select num from a where exists(select 1 from b where num=a.num)（正确） 11、尽可能的使用 varchar 代替 char ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux环境下配置redis以及redis相关操作]]></title>
    <url>%2F2019%2F03%2F22%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%85%8D%E7%BD%AEredis%E4%BB%A5%E5%8F%8Aredis%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言此文章主要是在Linux服务器上配置redis、使用redis进行相关操作以及使用RedisDesktopManager进行远程连接。Redis是当前比较热门的NOSQL系统之一，它是一个key-value存储系统。和Memcache类似，但很大程度补偿了Memcache的不足，它支持存储的value类型相对更多，包括string、list、set、zset和hash。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作。在此基础上，Redis支持各种不同方式的排序。和Memcache一样，Redis数据都是缓存在计算机内存中，不同的是，Memcache只能将数据缓存到内存中，无法自动定期写入硬盘，这就表示，一断电或重启，内存清空，数据丢失。所以Memcache的应用场景适用于缓存无需持久化的数据。而Redis不同的是它会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，实现数据的持久化。 一、在Linux服务器上安装Redis1、在linux环境中下载并解压redis安装包,将安装包解压到/usr/local目录,命令行执行以下命令12wget http://download.redis.io/releases/redis-4.0.7tar -xzf redis-4.0.7.tar.gz -C /usr/local 2、执行make对Redis解压后文件进行编译1make 3、编译成功后，进入src文件夹，执行make install进行Redis安装12cd srcmake install 二、Reids的部署1、为了方便管理，将Redis文件中的conf配置文件和常用命令移动到统一文件中去12mdkir /usr/local/rediscp /usr/local/redis-4.0.7/redis.conf /usr/local/redis 2、修改redis配置文件（/usr/local/redis/redis.conf），以下是需要修改的内容1234vim /usr/local/redis/redis.confdaemonize yes(后台开启Redis,默认不开启)bind 127.0.0.1(注释掉主机地址，不然只能本地连接)requirepass 密码（登录Redis的密码，默认无密码） 3、Redis相关测试1、启动Redis12cd /usr/local/redis/bin./redis-server 2、连接Redis1./redis-cli 3、查看Redis是否在运行:1ps -ef|grep redis 4、修改配置文件后，由于之前已开启Redis进程，所以需要先关闭Redis进程，再重启Redis，方法如下：使用命令查看是否还处于监听状态1netstat -nlt 重新启动Redis12cd /usr/local/redis/bin./redis-server 5、测试本地连接是否成功6、测试RedisDesktopManager远程连接是否成功 三、Redis的相关操作Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 1、String（字符串）string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。 2、Hash（哈希）Redis hash 是一个键值(key=&gt;value)对集合。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 3、List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 4、Set（集合）Redis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 5、zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 6、具体相关操作Redis命令参考： http://redisdoc.com/persistence/save.html或者 http://www.runoob.com/redis/redis-data-types.html]]></content>
      <categories>
        <category>linux服务器</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux服务器搭建java环境]]></title>
    <url>%2F2019%2F03%2F14%2Flinux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BAjava%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[前言这篇文章主要讲解在linux服务器下配置java环境，我选择的阿里云的服务器，学生价9.5元一个月，对于学生来说比较便宜，推荐购买。学生服务器购买网址：https://promotion.aliyun.com/ntms/campus2017.html, 购买需要进行实名验证和学生验证，大部分服务都是linux服务器，下面的截图是我服务器的配置，推荐大家选择CentOS系统 服务器java环境手动搭建1、工具准备我们需要两款软件来连接服务器，一个叫Xshell，用来操控服务器，另一个叫Xftp，用来进行服务器和自己电脑之间的文件传输。 2、Xshell的连接Xhell连接很简单，首先输入自己服务器的ip地址，点击连接，出现然后输入用户名root，输入之前在服务器上设置的root密码，当屏幕出现Welcome to Alibaba Cloud Elastic Compute Service，表示登录成功 3、Xftp的连接Xftp的登录和XShell类似，输入服务器IP，输入用户名root，然后输入root密码，左边框为你自己电脑上的文件，右边框为linux服务器的文件，左右拖动来进行文件的上传和下载，在电脑先下载好linux版本的tomcat、nginx、jdk压缩包，下面的截图是我已经下载好并移到linux 4、JDK的安装将jdk压缩包解压到/usr/local目录下 使用vi编辑器打开/etc/profile配置文件，进行jdk环境变量的配置，在源文件的最后空一行，写入如下代码#set java environment JAVA_HOME=/usr/local/jdk1.8.0_161 CLASSPATH=.:$JAVA_HOME/lib.tools.jar PATH=$JAVA_HOME/bin:$PATH export JAVA_HOME CLASSPATH PATH 使用source /etc/profile重新加载/etc/profile配置文件，并使用java -version测试jdk是否安装成功,如果出现java version 版本号说明安装成功 5、tomcat的安装将tomcat压缩包解压到/usr/local目录下,进入/usr/local目录，以确认tomcat解压到/usr/local下，默认情况下，Linux不会开放端口号，需手动将8080端口号开放，并在服务器安全组中添加8080端口外网访问，随后进入tomcat的bin中开启tomcat服务，如果安装成功，在浏览器中输入服务器ip:8008可获取到tomcat的官方页面，截图如下 6、nginx的安装安装nginx依赖环境gcc,安装期间有提示一律选y yum install gcc-c++ yum -y install pcre pcre-devel yum -y install zlib zlib-devel yum -y install openssl openssl-devel 将nginx压缩包解压到/usr/local目录下 tar -xvf nginx-1.15.9.tar.gz -C /usr/local/ 进入nginx目录，编译并安装nginx ./configure make make install 开放linux的对外访问端口80，在默认情况下，Linux不会开放端口号，需要手动开放端口号。 /sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEPT 进入nginx/sbin目录，并启动nginx cd /usr/local/nginx/sbin ./nginx 如果启动成功，将会出现下面的截图页面 7、mysql的安装安装依赖 yum -y install perl perl-devel autoconf libaio 把下载的安装包解压到/usr/local下，并改名为mysql tar -xvf mysql-5.6.43-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ cd /usr/local mv mysql-5.6.43-linux-glibc2.12-x86_64.tar.gz mysql 添加系统mysql组合mysql用户 groupadd mysql useradd -r -g mysql -s /bin/false mysql 进入安装mysql软件目录，修改目录拥有者为mysql用户 cd mysql/ chown -R mysql:mysql ./ 安装数据库 ./scripts/mysql_install_db --user=mysql 修改当前目录拥有者为root用户 chown -R root:root ./ 修改当前data目录拥有者为mysql用户 chown -R mysql:mysql data 启动mysql服务和添加开机启动mysql服务： cp support-files/mysql.server /etc/init.d/mysql 启动mysql服务，如果启动失败，需要创建一个文件 service mysql start touch /var/log/mariadb.log(失败才执行) 修改mysql的root用户密码，root初始密码为空的： ./bin/mysqladmin -u root password 密码 把mysql客户端放到默认路径 ln -s /usr/local/mysql/bin/mysql /usr/local/bin/mysql 连接数据库,成功连接如下图： mysql -uroot -p 8、将项目部署到linux服务器上将项目打包成.war格式使用Xftp工具将打包好的war包复制到tomcat下的webapps目录如何访问项目：http://ip地址:8080/项目名称,如下图 ​ ​]]></content>
      <categories>
        <category>linux服务器</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>jdk</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
</search>
